\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Project: Car detection on KITTI dataset}

\author{Muhammad Haris Ibrahim\\
{\tt\small muhammad.ibrahim@student.uni-luebeck.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  Detecting cars on the road is a critical task for autonomous driving. This must be done with high precision and in real-time. 
  In this paper, we develop a 2D car detector by fine tuning the YOLO-v3 object detector to detect cars on a scaled down version of KITTI dataset. 
  We achieved 50 percent mean Average Precision after finetuning YOLO-v3 model previously trained on MS COCO dataset. 
  Improvements to the training process are required to achieve better results.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Object detection is a critical task for autonomous driving. An autonomous vehicle must be able to detect and keep track of objects of interest in its environment. The classification of objects is not enough, their position and orientation in the environment must also be predicted. 
Another challenge is that these detections must be performed at high precision and recall but also in real-time on edge devices. The objects of interest can be lane line markings, traffic signs, vehicles, pedestrians and more. 

In this project, we focus only on detecting cars. Car detection is specially important in autonomous driving to avoid collisions and plan safe trajectories even at low levels of autonomy.

There are many deep learning based approaches for object detection. We adapt YOLO-v3 \cite{redmon2018yolov3}, a general purpose 2D object detector based on a Convolutional Neural Network (CNN) architecture. For training and evaluating our car detector, we will use a simplified version of KITTI dataset containing cars with 2D bounding box labels. \cite{KITTI}

YOLO-v3 is an improved version of YOLO \cite{redmon2016yolov1} which achieves 
higher mean Average Precision (mAP) on 2D object detection datasets like MS-COCO \cite{mscoco}, while still being real-time.  

Transfer learning is then used to finetune it for car detection on our version of KITTI dataset. We obtained 50 percent mean Average Precision (mAP) on a validation set.
%-------------------------------------------------------------------------
\section{Related Work}
Since the success of Convolutional Neural Network (CNN) based approaches like Alexnet \cite{Alexnet} on image classification tasks, many similar approaches have been designed for object detection. Region Proposal CNNs (RCNN) \cite{ren2015faster} and its various variants use handcrafted algorithms to generate proposals for regions of interest. These proposals are then passed to a CNN which produces class and confidence predictions. The predictions are then fed back into the region proposal algorithm. This means that multiple passes are required to perform good predictions. This makes these networks slow at inference time. These multi-stage detectors are, therefore, not realtime. 
YOLO architecture \cite{redmon2016yolov1} and Single Shot Detector \cite{SSD_v1} solved this problem by performing the object detection in only one evaluation of the network. These are end to end CNNs. Out of these two, YOLO is the fastest. YOLO v3 \cite{redmon2018yolov3} is an improvement to the original YOLO architecture. YOLO architecture frames the object detection problem as a regression problem. It divides the image into a SxS grid, where S is the grid size. For each of these grid cells, there are three bounding box predictors in the output layer. A bounding box prediction consists of 4 coordinates of the bounding box. Each grid cell also includes a class prediction. YOLO-v3 also divides the images into SxS cell, but it produces class probabilities for each bounding box not just for each grid cell. Predictions are also made at three different scales. Prediction of the bounding box coordinates are also as offsets of bounding box priors. Since, many predictions are made for each object, Non Maximum Suppression (NMS) is applied to keep only the best predictions. A brief comparison of the performance of many object detection frameworks in shown in \ref{fig:detector_comparison}.

There are more recent and improved versions of YOLO such as \cite{yolov7} that are not developed by the orignal authors. Recent works like Detection Transformer \cite{DETR} use a CNN backbone with a transformer encoder and decode blocks. These approaches acheive higher mAP precision. They also remove the need for anchor boxes and NMS. Because of the limits of the dataset, limited time and GPU compute, we use the YOLOv3 architecture and not the newer ones.

\begin{figure}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{object_detector_comparisons.png}
	\end{center}
	\caption{speed (ms) vs accuracy (AP) on MS. COCO test set. YOLOv3 outforms many multstage and single stage object detectors in AP while being faster than all of them. This image is adapted from \cite{redmon2018yolov3} which is itself a version of image from \cite{retinaNet}}
	\label{fig:detector_comparison}
\end{figure}


They acheived this by dividing the image into a SxS grid of "predictors". Each predictor is responsible for generating predictions for an object, if the centre of the object bounding box lies in within the grid cell. The predictor generates a class probabilities, objectness confidence, \
and 4 bounding box coordinates. 


\section{Method}
In this section, you should describe the method you used to solve the problem in detail. Figures can help to explain the used method. An example how to insert a figure is given in Figure \ref{fig:long}. \\
Also, formulas often help to make the mathematical background clearer. How to insert them is given here: 
$$a + b + a = 2 \cdot a + b\,.$$
It is essential that the reader is able to understand your method according to this section.

% \begin{figure}[t]
% 	\begin{center}
% 		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% 		\includegraphics[width=1.0\linewidth]{AI-comic-funny.jpg}
% 	\end{center}
% 	\caption{Overview of method/ Network structure}
% 	\label{fig:long}
% \end{figure}

\section{Evaluation}
\subsection{Experimental Setup}
Here, you should describe the data you used for training, the hyper parameter settings, how you split the data into a training and a test set, etc.. Additionally, you should explain how you adapted your method to work well on your problem setup.




\subsection{Discussion}
In the discussion, you present and discuss the results your method achieved in the provided setups, why which set up works better, ... . The numbers (accuracy of your TEST set) should be given in a table, as e.g. Table \ref{tab:results}. You should also show exemplary images for which you achieved good results and examples, where the algorithm did not work as expected. Give possible explanations for both cases, e.g. ``what could be the reason for missing a Pedestrian? ``.

\begin{table}
	\begin{center}
		\begin{tabular}{|l|c|}
			\hline
			Method & Frobnability \\
			\hline\hline
			Theirs & Frumpy \\
			Ours & Makes one's heart Frob\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Results. Ours is better.}
	\label{tab:results}
\end{table}

\section{Conclusion and Future Work}
Finally, you shortly summarize what you achieved in your work and point out what could be done in the future to get even better results. 









%-------------------------------------------------------------------------

{\small
\bibliographystyle{ieee_fullname}
\bibliography{literature}
}

\end{document}
